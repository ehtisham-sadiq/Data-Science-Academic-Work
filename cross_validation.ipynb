{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction (35pts)\n",
    "In this notebook, you will classify emails as either spam or not spam using support vector machines. The full dataset consists 80k labeled emails. The labels are 1 if they are ham (not spam), and -1 if they are spam. The lines of the emails have already been slightly processed, such that different words are space delimited, however little other processing has occurred. \n",
    "\n",
    "## Preliminary notes\n",
    "1. You can not use scikit-learn. \n",
    "2. For this notebook, each proceeding part depends on the previous since we are building up a moderately sized data science pipeline. Verify your previous parts before proceeding onto the next. \n",
    "3. Similar the linear regression notebook of the previous assignment, you will need to use the tfidf function from the natural language processing notebook. You can download your notebook as a module and import it. If you're in 388 or you find that your implementation is too slow, copy the reference solution (its only 10 lines). \n",
    "4. As we move into more advanced algorithms and techniques, there will be more introductions of randomness. This means that some of the example outputs in the notebook contain some randomness, and will probably not match your results exactly. Verify your code by checking your properties/invariants or feeding in static inputs for which you can calculate the output. \n",
    "5. When writing pickle files to be read into Autolab, **write files with the binary flag**\n",
    "5. There is another contest at the end of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import scipy.optimize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Received: from NAHOU-MSMBX01V ([192.168.110.39]) by NAHOU-MSMBX05V.corp.enron.com with Microsoft SMTPSVC(5.0.2195.1600); Fri 29 Jun 2001 08:36:10 -0500 X-MimeOLE: Produced By Microsoft Exchange V6.0.4418.65 content-class: urn:content-classes:message Subject: FW: June 29 -- BNA Inc. Daily Labor Report MIME-Version: 1.0 Content-Type: text/plain; Content-Transfer-Encoding: binary Date: Fri 29 Jun 2001 08:36:09 -0500 Message-ID: <77DA52C3FD86904D8209C9750CD310B9C79BB3@NAHOU-MSMBX01V.corp.enron.com> X-MS-Has-Attach: X-MS-TNEF-Correlator: <77DA52C3FD86904D8209C9750CD310B9C79BB3@NAHOU-MSMBX01V.corp.enron.com> Thread-Topic: June 29 -- BNA Inc. Daily Labor Report Thread-Index: AcEAUaYbkE2KMWxCEdWxEABQi+MJ2QATr4SA From: \"Hu Sylvia\" <Sylvia.Hu@ENRON.com> To: \"Acevedo Felecia\" <Felecia.Acevedo@ENRON.com> \"Brown MeCole\" <MeCole.Brown@ENRON.com> \"Cash Michelle\" <Michelle.Cash@ENRON.com> \"Castellano Bonne\" <Bonne.Castellano@ENRON.com> \"Johnson Rick\" <Rick.Johnson@ENRON.com> \"Lynch Drew\" <Drew.C.Lynch@ENRON.com> \"Parker Gilda\" <Gilda.Parker@ENRON.com> \"Sullivan Kriste\" <Kriste.Sullivan@ENRON.com> \"Walker Simone Scott\" <SimoneScott.Walker@ENRON.com> \"White Bonnie\" <Bonnie.White@ENRON.com> Return-Path: Sylvia.Hu@ENRON.com User ID: enrondlr PW: bnaweb22 -----Original Message----- From: \"BNA Highlights\" <bhighlig@bna.com> Sent: Thursday June 28 2001 11:10 PM To: BNA Highlights Subject: June 29 -- BNA Inc. Daily Labor Report ______________________________ DAILY LABOR REPORT Highlights & Table of Contents June 29 2001 ______________________________ ISSN 1522-5968 Registered Web subscribers can access the full text of these articles by using the URL link supplied. Information about becoming a subscriber or signing up for a FREE Web trial is available at http://web.bna.com or call BNA Customer Relations at 1-800-372-1033 Mon. - Fri. 8:30 am - 7:00 pm (ET). __________ HIGHLIGHTS __________ D.C. CIRCUIT OVERTURNS NLRB DECISION ON WORKPLACE CONDUCT POLICIES A California firm\\'s two workplace policies barring abusive and threatening language and restricting solicitation and distribution did not constitute unfair labor practices the District of Columbia Circuit rules overturning a National Labor Relations Board decision (\"Adtranz ABB Daimler-Benz Transp. N.A. Inc. v. NLRB \"D.C. Cir. No. 00-1282 6/26/01). The board held the workplace conduct policies of Adtranz ABB Daimler-Benz Transportation N.A. Inc. interfered with employees\\' exercise of their rights under the National Labor Relations Act. The board ordered that a second election be held to determine whether Adtranz employees in Pittsburg Calif. wanted to be represented by the Machinists Automotive Trade District Lodge 190 of Northern California. The appeals court finds it lacks jurisdiction to consider Adtranz\\'s appeal of the order for a new election because it is not a final order. The court rejects the board\\'s view that the ban on abusive and threatening language was an unfair labor practice regardless of whether it actually chilled employees\\' exercise of their rights. Judge Sentelle finds the solicitation-distribution rule is legal because it \"only applies to conduct during working time and in the work place\" and \"applies across the board so it cannot be said to discriminate against unionization efforts or other protected activity.\" . . . Page AA-1 Text E-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5k3h4_ http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5e3e4_ WORKPLACE PANIC ATTACKS RENDER EMPLOYEE UNQUALIFIED UNDER ADA A former customer service worker for a Wisconsin power company fired after having panic attacks at work that likely stemmed from a head injury was unable to establish that her discharge was illegal under the Americans with Disabilities Act the Seventh Circuit rules (\"Emerson v. Northern States Power Co. \"7th Cir. No. 00-3746 6/26/01). Loretta M. Emerson failed to show she was a \"qualified individual\" under the act given that her anxiety attacks could interfere with her ability to answer customer phone calls about gas or electricity emergencies Judge Bauer finds affirming summary judgment to Northern States Power Co. Bauer also finds Emerson posed a direct threat to her employer because her job \"required prompt accurate handling of emergencies such as gas leaks and downed power lines that could pose significant danger to the public.\" . . . Page A-8 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5q2p6_ UAN DELEGATES VOTE UNANIMOUSLY TO AFFILIATE WITH AFL-CIO Delegates to the National Labor Assembly of the United American Nurses the labor arm of the American Nurses Association unanimously vote to affiliate with the AFL-CIO. If all goes as planned the affiliation of the 100000-member union will become effective July 1. However it appears one more hurdle may need to be cleared before the affiliation which has been in the works for more than a year becomes a reality. The ANA House of Delegates is scheduled to vote June 30 on a bylaw change that would require any state nurses\\' association that engages in collective bargaining to be a member of UAN within four years. This was a condition of the AFL-CIO granting a charter. It is unclear whether the bylaw change will pass and if it does not what happens to the affiliation agreement. . . . Page B-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5j7f6_ NEW YORK CITY PARTLY IMMUNE FROM BIAS LAW COURT FINDS In a case of first impression a federal trial court overturns a jury\\'s $1 million sexual harassment punitive damages award against New York City because it did not explicitly waive sovereign immunity from punitive damages under the city\\'s human rights ordinance (\"Katt v. New York City \"S.D.N.Y. No. 95 Civ. 8283 (GEL) 6/19/01). Rejecting the defendants\\' other post-verdict motions however Judge Lynch of the U.S. District Court for the Southern District of New York upholds the jury\\'s award of $400000 in compensatory damages. The court rejects the city\\'s affirmative defense that plaintiff Alli Katt failed to take advantage of the New York Police Department\\'s complaint process finding that she was intimidated into not doing so because of explicit and implicit pressure. The court also rejects the city\\'s late-raised challenge to the timeliness of the complaint. . . . Page A-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j4v7g9_ HOFFA LEEDHAM NOMINATED AS CANDIDATES FOR TEAMSTERS PRESIDENT In a noisy but orderly session of the International Brotherhood of Teamsters convention June 28 incumbent President James P. Hoffa and challenger Tom Leedham both are nominated for the union\\'s top office. Also nominated are current IBT Secretary-Treasurer Tom Keegel and Leedham running-mate Tom Gilmartin the head of IBT Local 559 in Connecticut. Delegate voting will occur late June 28 to determine if the candidates received the requisite 5 percent support for their names to be placed on the ballot for the fall election among the union\\'s 1.4 million members. That Hoffa will receive 5 percent of the delegates\\' votes is a foregone conclusion given his overwhelming support among the more than 1700 delegates. Voice votes on various resolutions and constitutional amendments appear to show that Leedham has the support of only a few hundred delegates in his second challenge to Hoffa for the union presidency. However Leedham and members of his campaign appear confident he will get the votes necessary to be placed on the ballot. Other members of Leedham\\'s slate earlier received more votes than necessary to be placed on the ballot. . . . Page C-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5g8z1_ OSHA READIES MANDATORY EMPLOYER SURVEY FOR DISTRIBUTION The week of July 2 will signal the launch of the Occupational Safety and Health Administration\\'s annual mandatory survey of more than 80000 employers nationwide to gather injury and illness data to better target high-hazard employers. The agency however will not include construction employers in the upcoming survey. The 2001 survey program \"improves OSHA\\'s ability to identify and target our efforts on the more hazardous workplaces\" acting OSHA Administrator R. Davis Layne says. The survey requests calendar year 2000 data. OSHA inspections based on the 2000 data will not be launched until January or February of 2002 the OSHA spokesman says. Employers will be asked to supply figures for average employment hours worked and a summary of the job-related injuries and illnesses that occurred at their work sites in 2000 the agency says. The surveys must be returned even if a surveyed employer recorded no occupational injuries or illnesses for 2000 the agency says. . . . Page A-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5h0x6_ ______________ TODAY\\'S EVENTS ______________ GDP: Second revision of gross domestic product for the first quarter released 8:30 a.m. Commerce Department. ________________ ALSO IN THE NEWS ________________ LAYOFFS: Reflecting the weakened labor market both the number of mass layoff events and the number of workers involved were significantly higher during the first five months of this year than in the same period of 2000. The layoff events reported for May alone involved a total of 155759 workers. . . . Page D-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5j8p4_ UNEMPLOYMENT INSURANCE: New UI claims filed with state agencies fell for the third consecutive week down 16000 to a seasonally adjusted total of 388000 for the week ended June 23. . . . Page D-6 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5h5h2_ HEALTH CARE EMPLOYEES: Several factors are combining to constrain the current supply of nurses in the United States a key factor being job dissatisfaction according to the General Accounting Office. . . . Page A-5 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5c8p3_ ENTERTAINMENT: Members of six unions representing 25000 workers at Walt Disney World in Orlando Fla. are scheduled to vote July 6 on a new three-year contract with the Walt Disney World Co. . . . Page A-6 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5m9f3_ ____ TEXT ____ UNFAIR LABOR PRACTICES: District of Columbia Circuit decision in \"Adtranz ABB Daimler-Benz Transportation N.A. Inc. v. NLRB. \". . . Page E-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5e3e4_ _________________ TABLE OF CONTENTS _________________ LEADING THE NEWS UNFAIR LABOR PRACTICES Overturning NLRB D.C. Circuit finds California firm\\'s workplace policies barring abusive and threatening language and restricting solicitation and distribution did not constitute unfair labor practices . . . Page AA-1 Text E-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5k3h4_ http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5e3e4_ ____ NEWS ____ DISABILITIES Seventh Circuit rules former customer service worker at Wisconsin power company fired after having panic attacks at work likely related to head injury failed to establish she was \"qualified individual\" under ADA . . . Page A-8 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5q2p6_ DISCRIMINATION House Judiciary Committee expects to vote on bill (H.R. 7) that would allow religious organizations to compete for federal grants including proposed amendment from panel chairman that would narrow ability of such organizations to discriminate in employment . . . Page A-12 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5m7q5_ ECONOMIC OUTLOOK UCLA Anderson Forecast finds California facing most troublesome period since early 1990s predicts sharply lower growth through at least 2002 . . . Page A-3 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j4m8v2_ ENTERTAINMENT Members of six unions representing 25000 workers to vote July 6 on new three-year contract with Walt Disney World Co. . . . Page A-6 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5m9f3_ HEALTH CARE Senate Democrats prevail on amendment challenges to patients\\' bill of rights legislation but prospects for wrapping up work on bill by close of Senate business for night appear dim . . . Page A-10 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5v0u5_ HEALTH CARE EMPLOYEES GAO cites job dissatisfaction as top among several factors combining to constrain current supply of nurses in United States . . . Page A-5 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5c8p3_ HELP-WANTED ADS Conference Board finds demand for labor declined in May as help-wanted advertising index fell by 5 percentage points from previous month to 60 percent . . . Page A-5 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5j9t7_ SAFETY & HEALTH Official says Labor Department will go forward with Clinton administration rule changing the way some 1.3 million worksites are required to record employee injuries and illnesses . . . Page A-9 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5r0g7_ OSHA announces July 2 launch of annual mandatory survey gathering injury and illness data from more than 80000 employers to better target high-hazard employers . . . Page A-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5h0x6_ Study finds consequences of feeling overworked can affect employees\\' job performance and lead to poorer health . . . Page A-6 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5n9r1_ SEX DISCRIMINATION Seventh Circuit affirms grant of summary judgment to employer of female manager passed over for promotion and transferred to another facility finding that she failed to establish similarly situated male was treated more favorably . . . Page A-4 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j4g0q8_ SEXUAL HARASSMENT Federal judge overturns jury\\'s award of $1 million in punitive damages against New York City because city did not explicitly waive sovereign immunity under human rights ordinance . . . Page A-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j4v7g9_ STEEL Geneva Steel and United Steelworkers reach tentative one-year contract covering 1500 workers at company\\'s Vineyard Utah mill . . . Page A-13 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5g8h7_ TRADE In letter to President Bush twenty-five House Democrats denounce direction House GOP leadership and administration are taking on trade . . . Page A-7 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5q2j2_ WORKFORCE REDUCTIONS John Deere & Co. announces plans to cut salaried workforce in United States by 8 percent or 1250 jobs . . . Page A-13 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5d1v8_ ______________ SPECIAL REPORT ______________ HEALTH CARE EMPLOYEES Delegates to second annual National Labor Assembly of United American Nurses labor arm of American Nurses Association vote unanimously to affiliate with AFL-CIO . . . Page B-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5j7f6_ _________________ CONVENTION REPORT _________________ TEAMSTERS IBT delegates nominate incumbent President James P. Hoffa and challenger Tom Leedham for union\\'s top office in noisy but orderly session . . . Page C-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5g8z1_ _____________ ECONOMIC NEWS _____________ LAYOFFS Labor Department\\'s Bureau of Labor Statistics reports both number of mass layoff events and number of workers involved were significantly higher during first five months of this year than during same period last year reflecting weakened labor market . . . Page D-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5j8p4_ UNEMPLOYMENT INSURANCE Labor Department\\'s Employment and Training Administration reports new unemployment insurance claims filed with state agencies fell for third consecutive week down 16000 to seasonally adjusted total of 388000 for week ended June 23 . . . Page D-6 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5h5h2_ ____ TEXT ____ UNFAIR LABOR PRACTICES District of Columbia Circuit decision in \"Adtranz ABB Daimler-Benz Transportation N.A. Inc. v. NLRB\" . . . Page E-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5e3e4_ ______________ TABLE OF CASES ______________ Adtranz ABB Daimler-Benz Transp. N.A. Inc. v. NLRB (D.C. Cir.) . . . Page AA-1 text E-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5k3h4_ http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5e3e4_ Emerson v. Northern States Power Co. (7th Cir.) . . . Page A-8 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j5q2p6_ Hoffman-Dombrowski v. Arlington Int\\'l Racecourse Inc. (7th Cir.) . . . Page A-4 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j4g0q8_ Katt v. New York City (S.D.N.Y.) . . . Page A-1 http://pubs.bna.com/ip/BNA/dlr.nsf/id/a0a4j4v7g9_ __________ Daily Labor Report (ISSN 1522-5968) Highlights are published daily by The Bureau of National Affairs Inc. 1231 25th St. NW Washington DC 20037. For account information and changes contact 1-800-372-1033 (M-F 8:30 am-7:00 pm ET) To request retransmission or to order a copy of the summarized article contact 1-800-452-7773 or e-mail bnaplus@bna.com. For copyright guidelines go to http://www.bna.com/corp/copyright. Copyright (c) 2001 by The Bureau of National Affairs Inc. Washington D.C. 20037. Use of this service is subject to the terms and conditions of the license agreement with BNA. Unauthorized access or distribution is prohibited. \\n',\n",
       " 'Received: from nahou-msmbx03v.corp.enron.com ([192.168.110.40]) by NACAL-MSMBX01V with Microsoft SMTPSVC(5.0.2195.1600); Fri 29 Jun 2001 08:37:05 -0600 X-MimeOLE: Produced By Microsoft Exchange V6.0.4418.65 content-class: urn:content-classes:message Subject: NGX failover plan. Date: Fri 29 Jun 2001 09:37:04 -0500 MIME-Version: 1.0 Content-Type: text/plain; Content-Transfer-Encoding: binary Message-ID: <32B0D9A2EB95F745BF57B5FFB7A231CB1FBE48@NAHOU-MSMBX03V.corp.enron.com> X-MS-Has-Attach: X-MS-TNEF-Correlator: <32B0D9A2EB95F745BF57B5FFB7A231CB1FBE48@NAHOU-MSMBX03V.corp.enron.com> Thread-Topic: NGX failover plan. Thread-Index: AcEAqPcDVIHC9ZktSYOJgjEIiif3YQ== From: \"Webb Jay\" <Jay.Webb@ENRON.com> To: \"Lambie Chris\" <Chris.Lambie@ENRON.com> Cc: \"Mckay Jonathan\" <Jonathan.McKay@ENRON.com> Return-Path: Jay.Webb@ENRON.com Hi Chris Tonight we are rolling out a new report. Currently only you and Jon have access to it. You\\'ll select it off the main reports menu. It will say \"NGX Download report\". When you launch it it will show the data on screen like all other reports. There is a button at the bottom that says \"Export\". Clicking it will give you several selections. Choose the one that says \"Save to NGX XML\". You can then save it as a file which can be mailed to NGX. Let me know if there is anyone else who needs access. Thanks! -jay \\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "with open(\"X1.txt\") as f:\n",
    "    emails = f.readlines()\n",
    "labels = np.loadtxt(\"y1.txt\")\n",
    "emails[0:2]\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "from natural_language_processing import tfidf\n",
    "features, all_words = tfidf(emails)\n",
    "# AUTOLAB_IGNORE_STOP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification (15pts)\n",
    "Recall the support vector machine (SVM) from slide 17 of linear classification. Since it is such a straightforward algorithm, you will implement it below. \n",
    "\n",
    "### Grading\n",
    "* 2pts - correct objective\n",
    "* 5pts - correct gradient\n",
    "* 8pts - correct prediction after training\n",
    "\n",
    "### Specifications\n",
    "1. If you do not use matrix operations, your code will be **very slow**. Every function in here can be implemented in 1 or 2 lines using matrix equations, and the only for loop you need is the training loop for gradient descent. **If your code is slow here, it will be extremely slow in the next section when doing parameter search**.\n",
    "2. You should train your SVM using gradient descent as described in the slides. Your objective value should also mimic that of the slides. \n",
    "3. Since this is a convex function, your gradient steps should always decrease your objective. A simple check when writing these optimization procedures is to print your objectives and verify that this is the case (or plot them with matplotlib).\n",
    "4. You can also use scipy.optimize.check_grad to numerically verify the correctness of your gradients. \n",
    "5. For the unlikely boundary case where your hypothesis outputs 0, we will treat that as a positive prediction. \n",
    "6. Be careful of numpy.matrix objects which are constrained to always have dimension 2 (scipy operations will sometimes return this instead of an ndarray). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful tricks for debugging: \n",
    "1. Use very simple inputs (i.e. small vectors of ones) and compare the output of each function with a hand calculation. \n",
    "2. One way to guarantee your gradient is correct is to verify it numerically using a derivative approximation. You can read more about numerical differentiation methods here (https://en.wikipedia.org/wiki/Finite_difference) but for your purposes, you can use scipy.optimize.check_grad to do the numerical checking for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4888909.48753\n",
      "<class 'numpy.ndarray'>\n",
      "2.98 s ± 134 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "#Verify the correctness of your code on small examples\n",
    "#y0 = np.random.randint(0,2,5)*2-1\n",
    "#X0 = np.random.random((5,10))\n",
    "t0 = np.random.random(features.shape[1])\n",
    "np.random.seed(100)\n",
    "y0 = np.random.randint(0,2,6)*2-1\n",
    "X0 = np.random.randint(10,size=(6,3))\n",
    "#t0 = np.random.randint(1,3, size=3)\n",
    "svm0 = SVM(features,labels, 1e-4)\n",
    "svm0.theta = t0\n",
    "#print(svm0.train(10,1,verbose=False))\n",
    "#print(sp.diags(y0)*X0)\n",
    "print(svm0.objective(features,labels))\n",
    "#print(svm0.predict(features))\n",
    "print(type(X0))\n",
    "#def obj(theta):\n",
    " #   pass\n",
    "\n",
    "#d#ef grad(theta):\n",
    "  #  pass\n",
    "\n",
    "#scipy.optimize.check_grad(obj, grad, t0)\n",
    "\n",
    "%timeit svm0.train(niters=100, learning_rate=1, verbose=True)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above small example, our solution gets a gradient error on the order of 1e-08 from scipy.optimize.check_grad. Your objective values should be monotonically decreasing. \n",
    "\n",
    "Once that works, try training your SVM on the tfidf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "# svm = SVM(...)\n",
    "# svm.train()\n",
    "# yp = svm.predict(...)\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gets the following results:\n",
    "* For 100 iterations, regularization 1e-4, and learning rate 1.0, our solution is able to achieve perfect training classification accuracy (100% accuracy on the training data)\n",
    "* Training for 100 iterations takes about 2.13 seconds (measured using %timeit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross validation and Parameter Grid Search (15pts)\n",
    "As you may have noticed, there are parameters in the SVM learning algorithm that we chose somewhat arbitrarily: the regularization parameter and the learning rate (also technically the number of iterations for the learning algorithm, but you'll only consider the first two for simplicity). \n",
    "\n",
    "We were also able to achieve perfect training accuracy with these random parameters. This should make you suspicious: we have an enormous amount of features so it would be extremely easy to overfit to the data, so our model may not generalize well. \n",
    "\n",
    "You will now evaluate and select parameters using cross validation and grid search.\n",
    "\n",
    "### Grading\n",
    "* 2pts correct blocks and test_block attributes\n",
    "* 8pts correct cross validation \n",
    "* 3pts correct grid search\n",
    "* 2pts correct test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ModelSelector:\n",
    "    \"\"\" A class that performs model selection. \n",
    "        Attributes:\n",
    "            blocks (list) : list of lists of indices of each block used for k-fold cross validation, e.g. blocks[i] \n",
    "            gives the indices of the examples in the ith block \n",
    "            test_block (list) : list of indices of the test block that used only for reporting results\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, P, k, niters):\n",
    "        \"\"\" Initialize the model selection with data and split into train/valid/test sets. Split the permutation into blocks \n",
    "            and save the block indices as an attribute to the model. \n",
    "            Args:\n",
    "                X (array_like) : array of features for the datapoints\n",
    "                y (vector) : 1D numpy array containing the output labels for the datapoints\n",
    "                P (vector) : 1D numpy array containing a random permutation of the datapoints\n",
    "                k (int) : number of folds\n",
    "                niters (int) : number of iterations to train for\n",
    "        \"\"\"\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.P=P\n",
    "        self.k=k\n",
    "        self.niters=niters\n",
    "        \n",
    "        self.P=list(self.P)\n",
    "        self.blocksize=int(np.ceil(len(self.P)/(self.k + 1)))\n",
    "        num_el_test=len(self.P)- self.blocksize*self.k\n",
    "        self.test_block=self.P[-num_el_test:]\n",
    "        \n",
    "        self.training=list(set(self.P)-set(self.test_block))\n",
    "        \n",
    "        self.blocks=[]\n",
    "        counter=0\n",
    "        while (counter<len(P)-self.blocksize):\n",
    "            self.blocks.append(self.P[counter:counter+self.blocksize])\n",
    "            counter=counter+self.blocksize\n",
    "            \n",
    "        \n",
    "        \n",
    "        pass\n",
    "\n",
    "    def cross_validation(self, lr, reg):\n",
    "        \"\"\" Given the permutation P in the class, evaluate the SVM using k-fold cross validation for the given parameters \n",
    "            over the permutation\n",
    "            Args: \n",
    "                lr (float) : learning rate\n",
    "                reg (float) : regularizer parameter\n",
    "            Output: \n",
    "                (float) : the cross validated error rate\n",
    "        \"\"\"\n",
    "        error=0\n",
    "        for holdout in self.blocks:\n",
    "            #https://stackoverflow.com/questions/2070643/subtracting-two-lists-in-python?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "            exc_holdout = list(set(self.training)-set(holdout))\n",
    "            svm0=SVM(self.X[exc_holdout,:],self.y[exc_holdout],reg)\n",
    "            svm0.train(niters=self.niters, learning_rate=lr, verbose=False)\n",
    "            \n",
    "            predictions=svm0.predict(self.X[holdout,:])\n",
    "            \n",
    "            error+=np.sum(predictions!=self.y[holdout])\n",
    "            \n",
    "            \n",
    "        return error/(len(self.blocks)*self.blocksize)\n",
    "            \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def grid_search(self, lrs, regs):\n",
    "        \"\"\" Given two lists of parameters for learning rate and regularization parameter, perform a grid search using\n",
    "            k-wise cross validation to select the best parameters. \n",
    "            Args:  \n",
    "                lrs (list) : list of potential learning rates\n",
    "                regs (list) : list of potential regularizers\n",
    "            Output: \n",
    "                (lr, reg) : 2-tuple of the best found parameters\n",
    "        \"\"\"\n",
    "        params=[[self.cross_validation(lr,reg),float(lr),float(reg)] for lr in lrs for reg in regs]\n",
    "        best_params=min(params)\n",
    "        \n",
    "        return (best_params[1],best_params[2])\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def test(self, lr, reg):\n",
    "        \"\"\" Given parameters, calculate the error rate of the test data given the rest of the data. \n",
    "            Args: \n",
    "                lr (float) : learning rate\n",
    "                reg (float) : regularizer parameter\n",
    "            Output: \n",
    "                (err, svm) : tuple of the error rate of the SVM on the test data and the learned model\n",
    "        \"\"\"\n",
    "        svm0=SVM(self.X[self.training],self.y[self.training],reg)\n",
    "        svm0.train(niters=self.niters, learning_rate=lr, verbose=False)\n",
    "        pred=svm0.predict(self.X[self.test_block])\n",
    "        \n",
    "        err=np.sum(pred!=self.y[self.test_block])/len(pred)\n",
    "        \n",
    "        return (err,svm0)\n",
    "        \n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "How can we evaluate our choice of parameters? One way is to perform k-fold cross validation, which operates as follows \n",
    "\n",
    "1. We split the data into k+1 randomly selected but uniformly sized pieces, and set aside one block for testing\n",
    "2. For each of the remaining k parts, we train the model on k-1 parts and validate our model on the heldout part. \n",
    "3. This gives k results, and the average of these runs gives the final result\n",
    "\n",
    "The idea is that by holding out part of the dataset as validation data, we can train and measure our generalization ability. Note the key fact here: the training does not see the validation data at all, which is why it measures generalization! Randomizing the groups removes bias from ordering (i.e. if these results occurred in chronological order, we don't want to train on only Monday's results to predict on Wednesday's results), and averaging over the groups reduces the variance. \n",
    "\n",
    "In this problem, we will use classification error rate as our result metric (so the fraction of times in which our model returns the wrong answer). Calculating this value via k-fold cross validation gives us a measure of how well our model generalizes to new data (lower error rate is better). \n",
    "\n",
    "### Specification\n",
    "1. Break the examples in k+1 groups as follows: \n",
    "    * break the permutation into blocks of size $\\text{ceil}\\left(\\frac{n}{k+1}\\right)$ (the last block may be shorter than the rest)\n",
    "    * set aside the k+1th group as the testing block, and use the remaining k blocks for cross validation\n",
    "    * use the permutation as indices to select the rows that correspond to that block\n",
    "    * Example: k=2, P=[1,3,2,4,5,6] sets aside [5,6] as the test set, and break the remaining permutation into [[1,3],[2,4]] so the blocks of data for validation are X[[1,3],:] and X[[2,4],:]\n",
    "    * the order of the indices in the blocks should match the order in the original permutation\n",
    "2. For each group k, train the model on all other datapoints, and compute the error rate on the held-out group. \n",
    "3. Return the average error rate over all k folds, along \n",
    "\n",
    "You can try it on the random dataset just to make sure it works, but you won't get anything meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010317936412717457"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "MS0 = ModelSelector(features, labels,np.random.permutation(features.shape[0]), 5, 100)\n",
    "MS0.cross_validation(1, 1e-4)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running this on the tfidf features. Can you achieve the same performance on the validation dataset as you did on the training data set? Remember to use a random permutation (you'll get noticeably different results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "# MS0 = ...\n",
    "# MS0.cross_validation(...)\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation returns results with mean classification error 0.01169 and standard deviation 0.0092 (over 10 different permutations). The parameters we used are k=5 folds for learning rate 1 and regularization 1e-4, when run for 100 iterations. Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "Now, we have a means of evaluating our choice of parameters. We can now combine this with a grid search over parameters to determine the best combination. Given two lists of parameters, we compute the classification error using k-fold cross validation for each pair of parameters, and output the parameters that produces the best validation result. \n",
    "\n",
    "### Specification\n",
    "1. Select the pair of hyperparamers that produces the smallest k-fold validation error. \n",
    "2. Train a new model using all the training and validation data\n",
    "3. Report the classification accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-daab57638a1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mMS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelSelector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#%timeit lr, reg = MS.grid_search([0.1,1,10], [0.01,0.1,1,10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# AUTOLAB IGNORE_STOP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "# MS = ModelSelector(...)\n",
    "# lr, reg = MS.grid_search(...)\n",
    "# print(lr, reg)\n",
    "# print(MS.test(lr,reg))\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "MS = ModelSelector(features, labels, np.arange(features.shape[0]), 4, 100)\n",
    "#%timeit lr, reg = MS.grid_search([0.1,1,10], [0.01,0.1,1,10])\n",
    "print(lr, reg)\n",
    "print(MS.test(1,0.1))\n",
    "# AUTOLAB IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can try it on the randomized small example just to make sure your code runs, however it won't produce any sort of meaningful result. On our implementation, performing a grid search on learning rates [0.1, 1, 10] and regularization [0.01, 0.1, 1, 10] with 100 iterations for training results in a final test error rate of 0.0232 and selects a learning rate of 1, and a regularization parameter of 0.1. Our implementation takes about 1 minute and 7 seconds to perform the grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Compression (0pts)\n",
    "While you are able to get decent results using an SVM and basic tf-idf features, there are 2 main problems here:\n",
    "1. The actual dataset is 8x larger than the one that you load at the start\n",
    "2. The number of features is extremely bloated and consumes a lot of space and computing power for a binary classification problem\n",
    "\n",
    "So the above methodology would actually take a lot of time and memory to run on the full dataset. Following the example you did in the text classification notebook, we would need to save the tf-idf matrix for the entire training dataset (which is enormous), and then use that to generate features on new examples. \n",
    "\n",
    "One way to tackle this is to generate fewer, but effective, features. For example, instead of generating full tf_idf features for every single word in every email, we can instead try to focus on keywords that frequently only occur in spam email. This was hinted at in the previous contest, but was not emphasized enough. \n",
    "\n",
    "This problem is not graded if you wish to create different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def find_frequent_indicator_words(docs, y, threshold):\n",
    "       \n",
    "    def clean(email):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        email=email.translate(translator)\n",
    "        email = email.lower()\n",
    "        words=re.split(\"\\W+\", email)\n",
    "        return words\n",
    "    \n",
    "    spam_words=[]\n",
    "    ham_words=[]\n",
    "    for email,label in zip(docs,y):\n",
    "        if label==1:\n",
    "            spam_words=spam_words+clean(email)\n",
    "        elif label==0:\n",
    "            ham_words=ham_words + clean(email)\n",
    "    \n",
    "    return spam_words,ham_words\n",
    "            \n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "#print(clean(\"hi my name! PRANAV hi\"))\n",
    "s,h = find_frequent_indicator_words(emails, labels, 50)\n",
    "#with open('student_data.pkl', 'wb') as f:\n",
    " #   pickle.dump((s,h), f)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712166"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gets 2422 spam words and 290 ham words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Spam Detection (5pts)\n",
    "\n",
    "Your goal here is to get at least 80% accuracy on spam detection in an efficient manner. If you are unsure of what to do, one way is to use the frequent indicator words implemented above and generate 2 features per emails: the number of spam indicator words and the number of ham indicator words for a total of two features. This is a huge dimensionality reduction!\n",
    "\n",
    "Of course, you don't have to do this. As long as you achieve at least 80% accuracy with your features you will receive the base credit for this problem. You are allowed to submit supplemental files. See the Contest section for more details. Make sure these supplemental files make it into your tar file (update the Makefile if you use it). \n",
    "\n",
    "### Grading\n",
    "* 5pts 80% or higher accuracy within the constraints of Autolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email2features(emails):\n",
    "    \"\"\" Given a list of emails, create a matrix containing features for each email. \"\"\"\n",
    "    # with open('student_data.pkl', 'rb') as f:\n",
    "    #     data = pickle.load(f)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "small_features = email2features(emails)\n",
    "# MS = ModelSelector(...)\n",
    "# lr, reg = MS.grid_search(...)\n",
    "# print(lr, reg)\n",
    "# err, svm = MS.test(lr,reg)\n",
    "# print(err)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contest\n",
    "The contest here is straightforward: get the best accuracy level you can on the held out test dataset. You are allowed many things:\n",
    "1. You can upload a file named 'student_data.pkl' which can be of arbitrary format so long as it meets the Autolab file size submission constraint. You can use this, i.e. to store dictionaries of useful words (but are not limited to just this). \n",
    "2. You can upload a file named 'student_params.pkl' which contains a dictionary of parameters you'd like us to run when training your SVM, i.e. regularization parameter, learning rate, and niters. It should follow the format as shown in the next cell. \n",
    "3. When writing pkl files, make sure you pass the binary flag 'b' when writing the file or Autolab will be unable to read it. \n",
    "4. Add pkl files to your submitted tar file if you want them to be present on autolab. \n",
    "4. In addition to the X1.txt and y1.txt, there are 70k more emails (10k per file) that you can peruse for useful data. These are available on the website as a separate download. You are free to use any or all of them locally to learn your parameters and to determine what data to save. \n",
    "\n",
    "Reiterating: there are **70k more emails** that you can use to build your features on the course website. If you are serious about hitting the top of the leaderboard, go get them! We've only included the first 10k in the handout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example. Remember to add these files to your tar archive\n",
    "# AUTOLAB_IGNORE_START\n",
    "with open('student_data.pkl', 'wb') as f:\n",
    "    pickle.dump((s,h), f)\n",
    "    \n",
    "with open('student_params.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        \"lr\" : 1.0,\n",
    "        \"reg\" : 1e-4,\n",
    "        \"niters\" : 100\n",
    "    }, f)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
