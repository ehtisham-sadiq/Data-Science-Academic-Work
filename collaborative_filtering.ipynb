{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction (688 only)\n",
    "In this question, you'll build a basic recommendation system using collaborative filtering to make recommendations on a typical recommendation systems dataset, the MovieLens dataset. The purpose of this question is to become familiar with the internals of recommendation systems: both how they train and how they form recommendations. \n",
    "\n",
    "### Grading \n",
    "Your submission will be scored in the following manner: \n",
    "* process - 10pts\n",
    "* train - 15pts\n",
    "* recommend - 10pts\n",
    "\n",
    "## Collaborative Filtering by Matrix Factorization\n",
    "In collaborative filtering we wish to factorize our ratings matrix into two smaller feature matrices whose product is equal to the original ratings matrix. Specifically, given some partially filled ratings matrix $X\\in \\mathbb{R}^{m\\times n}$, we want to find feature matrices $U \\in \\mathbb{R}^{m\\times k}$ and $V \\in \\mathbb{R}^{n\\times k}$ such that $UV^T = X$. In the case of movie recommendation, each row of $U$ could be features corresponding to a user, and each row of $V$ could be features corresponding to a movie, and so $u_i^Tv_j$ is the predicted rating of user $i$ on movie $j$. This forms the basis of our hypothesis function for collaborative filtering: \n",
    "\n",
    "$$h_\\theta(i,j) = u_i^T v_j$$\n",
    "\n",
    "In general, $X$ is only partially filled (and usually quite sparse), so we can indicate the non-presence of a rating with a 0. Notationally, let $S$ be the set of $(i,j)$ such that $X_{i,j} \\neq 0$, so $S$ is the set of all pairs for which we have a rating. The loss used for collaborative filtering is squared loss:\n",
    "\n",
    "$$\\ell(h_\\theta(i,j),X_{i,j}) = (h_\\theta(i,j) - X_{i,j})^2$$\n",
    "\n",
    "The last ingredient to collaborative filtering is to impose an $l_2$ weight penalty on the parameters, so our total loss is:\n",
    "\n",
    "$$\\sum_{i,j\\in S}\\ell(h_\\theta(i,j),X_{i,j}) + \\lambda_u ||U||_2^2 + \\lambda_v ||V||_2^2$$\n",
    "\n",
    "For this assignment, we'll let $\\lambda_u = \\lambda_v = \\lambda$. \n",
    "\n",
    "## MovieLens rating dataset\n",
    "To start off, let's get the MovieLens dataset. This dataset is actually quite large (24+ million ratings), but we will instead use their smaller subset of 100k ratings. You will have to go fetch this from their website. \n",
    "\n",
    "* You can download the archive containing their latest dataset release from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip (last updated October 2016). \n",
    "* For more details (contents and structure of archive), you can read the README at http://files.grouplens.org/datasets/movielens/ml-latest-README.html\n",
    "* You can find general information from their website description located at http://grouplens.org/datasets/movielens/. \n",
    "\n",
    "For this assignment, we will only be looking at the ratings data specifically. However, it is good to note that there is additional data available (i.e. movie data and user made tags for movies) which could be used to improve the ability of the recommendation system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "movies=movies.set_index(movies['movieId'])\n",
    "27369 in movies['movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "ratings.head()\n",
    "ratings['movieId'].min()\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Matrix factorization requires that we have our ratings stored in a matrix of users, so the first task is to take the dataframe and convert it into this format. Note that in general, typically these matrices are extremely large and sparse (especially if you want to process the 24 million ratings), however for the purposes of this assignment a dense representation of this smaller dataset should fit on any machine. \n",
    "\n",
    "### Specification\n",
    "* Split the data by assigning the first $\\mathrm{floor}(9n/10)$ permuted entries to be the training set, and the remaining to be the testing set. Use the order given by the permutation. \n",
    "* Each row of the ratings matrix corresponds to a user. The first row of the matrix should correspond to the first user (by userID), and so on. This is because the set of user IDs already form a consecutive range of numbers. \n",
    "* Each column of the ratings matrix corresponds to a movie. The order of the columns doesn't matter, so long as the resulting list of movie names is accurate. This is because the set of movie IDs does not form a consecutive range of numbers. \n",
    "* Each user and movie that exists in the **ratings** dataframe should be present in the ratings matrix, even if it doesn't have any entries. We will only use the movies dataframe to extract the names of the movies. \n",
    "* Any entry that does not have a rating should have a default value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.linalg as la\n",
    "import matplotlib\n",
    "matplotlib.use(\"svg\")\n",
    "# AUTOLAB_IGNORE_START\n",
    "%matplotlib inline\n",
    "# AUTOLAB_IGNORE_STOP\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 9066) (671, 9066) ['Toy Story (1995)', 'Jumanji (1995)', 'Grumpier Old Men (1995)', 'Waiting to Exhale (1995)', 'Father of the Bride Part II (1995)']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import *\n",
    "def process(ratings, movies, P):\n",
    "    \"\"\" Given a dataframe of ratings and a random permutation, split the data into a training \n",
    "        and a testing set, in matrix form. \n",
    "        \n",
    "        Args: \n",
    "            ratings (dataframe) : dataframe of MovieLens ratings\n",
    "            movies (dataframe) : dataframe of MovieLens movies\n",
    "            P (numpy 1D array) : random permutation vector\n",
    "            \n",
    "        Returns: \n",
    "            (X_tr, X_te, movie_names)  : training and testing splits of the ratings matrix (both \n",
    "                                         numpy 2D arrays), and a python list of movie names \n",
    "                                         corresponding to the columns of the ratings matrices. \n",
    "    \"\"\"\n",
    "    u_l=len(ratings['userId'].unique())\n",
    "    m_l=len(ratings['movieId'].unique())\n",
    "   \n",
    "    indices=int(np.floor(9*len(P))/10)\n",
    "    \n",
    "    Train_val=ratings.iloc[P[0:indices],:]\n",
    "    Test_val=ratings.iloc[P[indices:],:]\n",
    "    \n",
    "    Train_mat=np.zeros((u_l,m_l))\n",
    "    Test_mat=np.zeros((u_l,m_l))\n",
    "\n",
    "    val_mat=range(1,m_l+1)\n",
    "   \n",
    "    un_mov=sorted(ratings['movieId'].unique())\n",
    "    map_dict= dict(zip(un_mov,[x for x in val_mat]))\n",
    "    map_dict=OrderedDict(map_dict)\n",
    "\n",
    "    for value in Train_val.itertuples():\n",
    "        Train_mat[value[1]-1,map_dict[value[2]]-1]=value[3]\n",
    "     \n",
    "    for value in Test_val.itertuples():\n",
    "        Test_mat[value[1]-1,map_dict[value[2]]-1]=value[3]\n",
    "   \n",
    "    movies=movies.set_index(movies['movieId'])\n",
    "    movie_name=[movies['title'][movie] for movie in un_mov]\n",
    "    \n",
    "    return (Train_mat,Test_mat,movie_name)\n",
    "\n",
    "    pass\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "X_tr, X_te, movieNames = process(ratings, movies, np.random.permutation(len(ratings)))\n",
    "print(X_tr.shape, X_te.shape, movieNames[:5])\n",
    "#print(np.random.permutation(len(ratings)))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, running this on the small MovieLens dataset using a random permutation gives the following result: \n",
    "    \n",
    "    (671L, 9066L) (671L, 9066L) ['Toy Story (1995)', 'Jumanji (1995)', 'Grumpier Old Men (1995)', 'Waiting to Exhale (1995)', 'Father of the Bride Part II (1995)']\n",
    "\n",
    "Your actual titles may vary depending on the random permutation given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Minimization for Collaborative Filtering\n",
    "Now we build the collaborative filtering recommendation system. We will use a method known as alternating least squares. Essentially, we will alternate between optimizing over $U$ and $V$ by holding the other constant. By treating one matrix as a constant, we get exactly a weighted least squares problem, which has a well-known solution. More details can be found in the lecture notes. \n",
    "\n",
    "### Specification\n",
    "* Similar to the softmax regression on MNIST, there is a verbose parameter here that you can use to print your training err and test error. These should decrease (and converge). \n",
    "* You can assume a dense representation of all the inputs. \n",
    "* You may find it useful to have an indicator matrix W where $W_{ij} = 1$ if there is a rating in $X_{ij}$. \n",
    "* You can initialize U,V with random values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error(X, U, V):\n",
    "    \"\"\" Compute the mean error of the observed ratings in X and their estimated values. \n",
    "        Args: \n",
    "            X (numpy 2D array) : a ratings matrix as specified above\n",
    "            U (numpy 2D array) : a matrix of features for each user\n",
    "            V (numpy 2D array) : a matrix of features for each movie\n",
    "        Returns: \n",
    "            (float) : the mean squared error of the observed ratings with their estimated values\n",
    "        \"\"\"\n",
    "    W=(X>0).astype(float)\n",
    "    pred=np.dot(U,V.T)\n",
    "    J=((X-pred)**2)*W\n",
    "    error=J.sum()/W.sum()\n",
    "    \n",
    "    return error\n",
    "    \n",
    "    pass\n",
    "\n",
    "def train(X, X_te, k, U, V, niters=51, lam=10, verbose=False):\n",
    "    \"\"\" Train a collaborative filtering model. \n",
    "        Args: \n",
    "            X (numpy 2D array) : the training ratings matrix as specified above\n",
    "            X_te (numpy 2D array) : the testing ratings matrix as specified above\n",
    "            k (int) : the number of features use in the CF model\n",
    "            U (numpy 2D array) : an initial matrix of features for each user\n",
    "            V (numpy 2D array) : an initial matrix of features for each movie\n",
    "            niters (int) : number of iterations to run\n",
    "            lam (float) : regularization parameter\n",
    "            verbose (boolean) : verbosity flag for printing useful messages\n",
    "            \n",
    "        Returns:\n",
    "            (U,V) : A pair of the resulting learned matrix factorization\n",
    "    \"\"\"\n",
    "    W=(X>0).astype(float)\n",
    "    \n",
    "    reg = lam*np.eye(k)\n",
    "    \n",
    "    print(\"Iteration \\t\\t|Train Error \\t\\t|Test Error\")\n",
    "        \n",
    "    for epoch in range(niters):\n",
    "        \n",
    "        \n",
    "        # Solving for U\n",
    "        for u,Wu in enumerate(W):\n",
    "            Wud=np.diag(Wu)\n",
    "            VWuYT=np.dot(V.T,np.dot(Wud,V))\n",
    "            LHS=VWuYT + reg\n",
    "            RHS=np.dot(V.T,np.dot(Wud,X[u].T))\n",
    "            \n",
    "            U[u]=(np.linalg.solve(LHS,RHS)).T\n",
    "            \n",
    "        # solving for V\n",
    "        for v,Wv in enumerate(W.T):\n",
    "            Wvd=np.diag(Wv)\n",
    "            UWvUT=np.dot(U.T,np.dot(Wvd,U))\n",
    "            LHS1=UWvUT + reg\n",
    "            RHS1=np.dot(U.T,np.dot(Wvd,X[:,v]))\n",
    "            \n",
    "            V[v]=np.linalg.solve(LHS1,RHS1).T\n",
    "        \n",
    "    return (U,V)\n",
    "        \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the recommendation system with a random initialization of U,V with 5 features and $\\lambda = 10$ results in the following output. Your results may vary depending on your random permutation.  \n",
    "\n",
    "    Iter |Train Err |Test Err  \n",
    "        0|    1.3854|    2.1635\n",
    "        5|    0.7309|    1.5782\n",
    "       10|    0.7029|    1.5078\n",
    "       15|    0.6951|    1.4874\n",
    "       20|    0.6910|    1.4746\n",
    "       25|    0.6898|    1.4679\n",
    "       30|    0.6894|    1.4648\n",
    "       35|    0.6892|    1.4634\n",
    "       40|    0.6891|    1.4631\n",
    "       45|    0.6891|    1.4633\n",
    "       50|    0.6891|    1.4636\n",
    "    Wall time: 7min 32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Finally, we need to be able to make recommendations given a matrix factorization. We can do this by simply taking the recommending the movie with the highest value in the estimated ratings matrix. \n",
    "\n",
    "### Specification\n",
    "* For each user, recommend the the movie with the highest predicted rating for that user that the user **hasn't** seen before. \n",
    "* Return the result in a list such that the ith element in this list is the recommendation for the user corresponding to the ith row of the ratings matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shaggy D.A., The (1976)', 'Village, The (2004)', 'Shaggy D.A., The (1976)', 'Village, The (2004)', 'Village, The (2004)', 'Village, The (2004)', 'Village, The (2004)', 'Village, The (2004)', 'Village, The (2004)', 'Village, The (2004)']\n"
     ]
    }
   ],
   "source": [
    "def recommend(X, U, V, movieNames):\n",
    "    \"\"\" Recommend a new movie for every user.\n",
    "        Args: \n",
    "            X (numpy 2D array) : the training ratings matrix as specified above\n",
    "            U (numpy 2D array) : a learned matrix of features for each user\n",
    "            V (numpy 2D array) : a learned matrix of features for each movie\n",
    "            movieNames : a list of movie names corresponding to the columns of the ratings matrix\n",
    "        Returns\n",
    "            (list) : a list of movie names recommended for each user\n",
    "    \"\"\"\n",
    "    Xpred=np.dot(U,V.T)\n",
    "    recomend_matrix=Xpred-X\n",
    "    movie_indices=recomend_matrix.argmax(axis=1)\n",
    "    recommended_movies=[movieNames[i] for i in movie_indices]\n",
    "    \n",
    "    return recommended_movies\n",
    "    pass\n",
    "    \n",
    "# AUTOLAB_IGNORE_START\n",
    "recommendations = recommend(X_tr, U, V, movieNames)\n",
    "print(recommendations[:10])\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gets the following results (we can see they are all fairly popular and well known movies that were recommended). Again your results will vary depending on the random permutation. \n",
    "\n",
    "    ['Shawshank Redemption, The (1994)', 'Shawshank Redemption, The (1994)', 'Shawshank Redemption, The (1994)', 'Shawshank Redemption, The (1994)', 'Shawshank Redemption, The (1994)', 'Shawshank Redemption, The (1994)', 'Godfather, The (1972)', 'Fargo (1996)', 'Godfather, The (1972)', \"Schindler's List (1993)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
